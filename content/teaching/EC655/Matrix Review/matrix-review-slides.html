<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Matrix Review</title>
    <meta charset="utf-8" />
    <meta name="author" content="Justin Smith" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link href="libs/remark-css/hygge.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Matrix Review
]
.subtitle[
## EC655
]
.author[
### Justin Smith
]
.institute[
### Wilfrid Laurier University
]
.date[
### Fall 2022
]

---




# Introduction

- Undergrad metrics normally uses scalar notation

  - More accessible for students without advanced math background

- At the graduate level, it is often taught using matrix algebra

- Some advantages to matrix notation

  - More compact
  
  - Easier to express some estimators

- In this section, we review matrix algebra essentials for econometrics

  - Not a comprehensive review

- We will switch between scalar and matrix notation in the course

  - Depending on which is clearer in each context
---

# Matrices and Vectors
## Vectors

- A .red[vector] is a matrix with one column or one row

- A .red[row vector] `\(\mathbf{a}\)` with *n* elements is

`$$\mathbf{a}= 
\begin{bmatrix}
a_{1}&amp; a_{2} &amp;\cdots &amp; a_{n} 
\end{bmatrix}$$`

- A .red[column vector] `\(\mathbf{a}\)` with *m* elements is

`$$\mathbf{a}= 
\begin{bmatrix}
a_{1}\\
a_{2}\\
\vdots \\
a_{m}
\end{bmatrix}$$`
---

# Matrices and Vectors
## Special Matrices

- A .red[Square Matrix] has the same number of rows and columns

`$$\mathbf{A}= 
\begin{bmatrix}
a_{11}&amp; a_{12} &amp;\cdots &amp; a_{1m} \\
a_{21}&amp; a_{22} &amp;\cdots &amp; a_{2m} \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
a_{m1}&amp; a_{m2} &amp;\cdots &amp; a_{mm} 
\end{bmatrix}$$`
 
- A .red[Diagonal Matrix] is a square matrix with zeroes for all off-diagonal elements
 
`$$\mathbf{A}=
\begin{bmatrix}
a_{11}&amp; 0&amp;\cdots &amp; 0 \\
0&amp; a_{22} &amp;\cdots &amp; 0 \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
0&amp; 0&amp;\cdots &amp; a_{mm} 
\end{bmatrix}$$`
---

# Matrices and Vectors

- The .red[Identity Matrix] is a square matrix with ones on the diagonal and zeroes on the off-diagonals

`$$\mathbf{I}= 
\begin{bmatrix}
1&amp; 0&amp;\cdots &amp; 0 \\
0&amp; 1 &amp;\cdots &amp; 0 \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
0&amp; 0&amp;\cdots &amp; 1 
\end{bmatrix}$$`
 
- The .red[Zero Matrix] is a matrix with zeroes for all elements
 
`$$\mathbf{0}= 
\begin{bmatrix}
0&amp; 0&amp;\cdots &amp; 0 \\
0&amp; 0 &amp;\cdots &amp; 0 \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
0&amp; 0&amp;\cdots &amp; 0
\end{bmatrix}$$`
---
# Matrix Operations
## Matrix Addition and Subtraction

- You can add and subtract matrices with the same dimensions

  - Matrices with different dimensions are not conformable for addition or subtraction

- The sum of matrices `\(\mathbf{A}\)` and `\(\mathbf{B}\)` with dimension `\(m \times n\)` is 

`$$\mathbf{A} + \mathbf{B}= 
\begin{bmatrix}
a_{11}&amp; a_{12} &amp;\cdots &amp; a_{1n} \\
a_{21}&amp; a_{22} &amp;\cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
a_{m1}&amp; a_{m2} &amp;\cdots &amp; a_{mn} 
\end{bmatrix}
+
\begin{bmatrix}
b_{11}&amp; b_{12} &amp;\cdots &amp; b_{1n} \\
b_{21}&amp; b_{22} &amp;\cdots &amp; b_{2n} \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
b_{m1}&amp; b_{m2} &amp;\cdots &amp; b_{mn} 
\end{bmatrix}$$`


`$$= 
\begin{bmatrix}
a_{11} + b_{11}&amp; a_{12} + b_{12} &amp;\cdots &amp; a_{1n}+ b_{1n} \\
a_{21} + b_{21}&amp; a_{22} + b_{22} &amp;\cdots &amp; a_{2n}+ b_{2n} \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
a_{m1} + b_{m1}&amp; a_{m2} +b_{m2} &amp;\cdots &amp; a_{mn}+ b_{mn} \\
\end{bmatrix}$$`
 
---

# Matrix Operations

- Similarly, the difference between matrices `\(\mathbf{A}\)` and `\(\mathbf{B}\)` is 

`$$\mathbf{A} - \mathbf{B}= 
\begin{bmatrix}
a_{11}&amp; a_{12} &amp;\cdots &amp; a_{1n} \\
a_{21}&amp; a_{22} &amp;\cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
a_{m1}&amp; a_{m2} &amp;\cdots &amp; a_{mn} 
\end{bmatrix}
-
\begin{bmatrix}
b_{11}&amp; b_{12} &amp;\cdots &amp; b_{1n} \\
b_{21}&amp; b_{22} &amp;\cdots &amp; b_{2n} \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
b_{m1}&amp; b_{m2} &amp;\cdots &amp; b_{mn} 
\end{bmatrix}$$`


`$$= 
\begin{bmatrix}
a_{11} - b_{11}&amp; a_{12} - b_{12} &amp;\cdots &amp; a_{1n}- b_{1n} \\
a_{21} - b_{21}&amp; a_{22} - b_{22} &amp;\cdots &amp; a_{2n}- b_{2n} \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
a_{m1} - b_{m1}&amp; a_{m2} -b_{m2} &amp;\cdots &amp; a_{mn}- b_{mn} \\
\end{bmatrix}$$`
---

# Matrix Operations

- The following rules apply to matrix addition and subtraction 

  - Commutativity
`$$\mathbf{A + B = B + A}$$`

  - Associativity
`$$\mathbf{A + (B + C) = (A+B) + C}$$`

- Effectively, both rules mean order does not matter	

  - Similar to scalar math


- For subtraction, replace plus sign with minus sign and same rules apply

---

# Matrix Operations
## Matrix Multiplication

- To multiply matrix `\(\mathbf{A}\)` and  `\(\mathbf{B}\)`, the number of columns in `\(\mathbf{A}\)`  must equal the number of rows in `\(\mathbf{B}\)` 

- Suppose matrix `\(\mathbf{A}\)` is `\(m \times n\)` and matrix  `\(\mathbf{B}\)` is `\(n \times p\)` 

- Define product as `\(\mathbf{C}\)`= `\(\mathbf{AB}\)`

  - The `\(ij\)` element of `\(\mathbf{C}\)` is the sum of the product of the corresponding elements along the `\(i\)`th row of `\(\mathbf{A}\)` and `\(j\)`th column of `\(\mathbf{B}\)`	
`$$c_{ij} = \sum_{k} a_{ik}b_{kj}$$`

  - The product matrix `\(\mathbf{C}\)` will have dimension `\(m \times p\)`
  
      - The number of rows of `\(\textbf{A}\)` and number of columns of `\(\textbf{B}\)`
      
---

# Matrix Operations

- The product `\(\mathbf{AB}\)` is
`$$\mathbf{AB}= 
\begin{bmatrix}
a_{11}&amp; a_{12} &amp;\cdots &amp; a_{1n} \\
a_{21}&amp; a_{22} &amp;\cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
a_{m1}&amp; a_{m2} &amp;\cdots &amp; a_{mn} 
\end{bmatrix}
\times
\begin{bmatrix}
b_{11}&amp; b_{12} &amp;\cdots &amp; b_{1p} \\
b_{21}&amp; b_{22} &amp;\cdots &amp; b_{2p} \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
b_{n1}&amp; b_{n2} &amp;\cdots &amp; b_{np} 
\end{bmatrix}$$`

`$$= 
\begin{bmatrix}
a_{11} b_{11} + a_{12} b_{21}  + \cdots + a_{1n} b_{n1}  &amp;a_{11} b_{12} + a_{12} b_{22}  + \cdots + a_{1n} b_{n2} &amp;\cdots&amp;a_{11} b_{1p} + a_{12} b_{2p}  + \cdots + a_{1n} b_{np}\\
a_{21} b_{11} + a_{22} b_{21}  + \cdots + a_{2n} b_{n1}  &amp;a_{21} b_{12} + a_{22} b_{22}  + \cdots + a_{2n} b_{n2} &amp;\cdots&amp;a_{21} b_{1p} + a_{22} b_{2p}  + \cdots + a_{2n} b_{np}\\
\vdots &amp;\ddots &amp; \vdots \\
a_{m1} b_{11} + a_{m2} b_{21}  + \cdots + a_{mn} b_{n1}  &amp;a_{m1} b_{12} + a_{m2} b_{22}  + \cdots + a_{mn} b_{n2} &amp;\cdots&amp;a_{m1} b_{1p} + a_{m2} b_{2p}  + \cdots + a_{mn} b_{np}\\
\end{bmatrix}$$`
---

## Matrix Operations

- As an illustration suppose we have the following matrices
`$$\mathbf{A}=
\begin{bmatrix}
1&amp; 2\\
3&amp; 4 \\
\end{bmatrix}
\mathbf{B}=
\begin{bmatrix}
5&amp;6&amp;7  \\
8&amp;9 &amp;10 
\end{bmatrix}$$`

- We can multiply `\(\mathbf{AB}\)` because `\(\mathbf{A}\)` has 2 columns, and `\(\mathbf{B}\)` has 2 rows 

- The product `\(\mathbf{C}\)` = `\(\mathbf{AB}\)` is

`$$\mathbf{C}= 
\begin{bmatrix}
1&amp; 2\\
3&amp; 4 \\
\end{bmatrix}
\times
\begin{bmatrix}
5&amp;6&amp;7  \\
8&amp;9 &amp;10 
\end{bmatrix}
=
\begin{bmatrix}
1 \times 5 + 2\times 8&amp;1 \times 6 + 2 \times 9 &amp; 1 \times 7 + 2 \times 10  \\
3 \times 5 + 4\times 8&amp;3 \times 6 + 4 \times 9 &amp; 3 \times 7 + 4 \times 10  
\end{bmatrix}$$`

`$$=
\begin{bmatrix}
21&amp; 24&amp; 27 \\
47&amp;54&amp;  61 
\end{bmatrix}$$`

---

# Matrix Operations
## Scalar Multiplication

- A .red[scalar] is a single real number

- You can also multiply a scalar by a matrix

- If `\(\gamma\)` is a scalar, and `\(\mathbf{A}\)`  is a matrix, then

`$$\mathbf{\gamma A}= \gamma 
\begin{bmatrix}
a_{11}&amp; a_{12} &amp;\cdots &amp; a_{1n} \\
a_{21}&amp; a_{22} &amp;\cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
a_{m1}&amp; a_{m2} &amp;\cdots &amp; a_{mn} 
\end{bmatrix}
=
\begin{bmatrix}
\gamma a_{11}&amp;\gamma  a_{12} &amp;\cdots &amp; \gamma a_{1n} \\
\gamma a_{21}&amp; \gamma a_{22} &amp;\cdots &amp; \gamma a_{2n} \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
\gamma a_{m1}&amp; \gamma a_{m2} &amp;\cdots &amp; \gamma a_{mn} 
\end{bmatrix}$$`
	
- You multiply the scalar by each element of the matrix	
	
---

# Matrix Operations
## Transpose

- The .red[transpose] of a matrix is one where the rows and columns are switched 

- Suppose matrix `\(\mathbf{A}\)` is 
`$$\mathbf{A}= 
\begin{bmatrix}
a_{11}&amp; a_{12} &amp;\cdots &amp; a_{1n} \\
a_{21}&amp; a_{22} &amp;\cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
a_{m1}&amp; a_{m2} &amp;\cdots &amp; a_{mn} 
\end{bmatrix}$$`

- Then its transpose `\(\mathbf{A'}\)` is

`$$\mathbf{A'}= 
\begin{bmatrix}
a_{11}&amp; a_{21} &amp;\cdots &amp; a_{m1} \\
a_{12}&amp; a_{22} &amp;\cdots &amp; a_{m2} \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
a_{1n}&amp; a_{2n} &amp;\cdots &amp; a_{mn} 
\end{bmatrix}$$`

---

# Matrix Operations

- The transpose has the following properties


`$$\mathbf{(A')' = A }$$`
`$$\mathbf{(\alpha A)' = \alpha A' }$$`
`$$\mathbf{(A + B)' = A' + B' }$$`
`$$\mathbf{(AB)' = B'A' }$$`

- There are additional rules for different types of matrices that we will cover below

---

# Matrix Operations
## Partitioned Matrix Multiplication

- You may sometimes want to break matrices into vectors before you multiply

- Multiplication works the same way, but notation can be cleaner and more intuitive

- Suppose we have the following matrices
`$$\mathbf{A}= 
\begin{bmatrix}
a_{11}&amp; a_{12} &amp;\cdots &amp; a_{1n} \\
a_{21}&amp; a_{22} &amp;\cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
a_{m1}&amp; a_{m2} &amp;\cdots &amp; a_{mn} 
\end{bmatrix}
\mathbf{B}= 
\begin{bmatrix}
b_{11}&amp; b_{12} &amp;\cdots &amp; b_{1p} \\
b_{21}&amp; b_{22} &amp;\cdots &amp; b_{2p} \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
b_{n1}&amp; b_{n2} &amp;\cdots &amp; b_{np} 
\end{bmatrix}$$`

- We are interested in the product `\(\mathbf{AB}\)`

---

# Matrix Operations

- Break these matrices into vectors conformable for multiplication

`$$\mathbf{A}=
\begin{bmatrix}
\mathbf{a_{1}}&amp;\mathbf{a_{2}} &amp; \cdots &amp; \mathbf{a_{n}}
\end{bmatrix}
\mathbf{B}= 
\begin{bmatrix}
\mathbf{b_{1}}\\
\mathbf{b_{2} }\\
\vdots  \\
\mathbf{b_{n}}
\end{bmatrix}$$`

- Where

`$$\mathbf{a_{1}}=
\begin{bmatrix}
a_{11}\\
a_{21}\\
\cdots\\
a_{m1}
\end{bmatrix}
\mathbf{b_{1}}=
\begin{bmatrix}
b_{11}&amp;b_{12} &amp; \cdots &amp; b_{1p}
\end{bmatrix}$$`

-  Multiply the vectors to get 
`$$\mathbf{AB} = \sum_{i=1}^{n} \mathbf{a_{i}b_{i}}$$`

---

# Matrix Operations

- This breaks the product `\(\mathbf{AB}\)` into the sum of `\(n\)` sub-matrices

  - Each sub-matrix is product of corresponding vectors
  
  - Also each sub-matrix will have dimension `\(m \times p\)`

- This will be useful for some econometric estimators we derive

  -  Makes notation simpler and more intuitive


- Again, note that you get the same answer as doing straight matrix multiplication

---

# Matrix Operations

- There are several useful properties for matrix (and scalar) multiplication 


`$$(\alpha + \beta)\mathbf{A} = \alpha \mathbf{A} + \beta\mathbf{A}$$`
`$$\alpha (\mathbf{A} +\mathbf{B}) =\alpha \mathbf{A} +\alpha\mathbf{B}$$`
`$$(\alpha\beta) \mathbf{A}  =\alpha(\beta \mathbf{A})$$`
`$$\alpha (\mathbf{A}\mathbf{B}) =(\alpha \mathbf{A}) \mathbf{B}$$`
`$$(\mathbf{A}\mathbf{B} )\mathbf{C} =\mathbf{A}(\mathbf{B} \mathbf{C})$$`
`$$\mathbf{A}(\mathbf{B} +\mathbf{C}) =\mathbf{A}\mathbf{B} +\mathbf{A} \mathbf{C}$$`
`$$(\mathbf{A}+\mathbf{B} )\mathbf{C} =\mathbf{A}\mathbf{C} +\mathbf{B} \mathbf{C}$$`
`$$\mathbf{A}\mathbf{I}  =\mathbf{I}\mathbf{A} = \mathbf{A}$$`
`$$\mathbf{A}\mathbf{0}  =\mathbf{0}\mathbf{A} = \mathbf{0}$$`
`$$\mathbf{A}\mathbf{B}  \neq\mathbf{B}\mathbf{A}$$`
---

# Matrix Operations
## Trace

- The .red[trace] of a square matrix is the sum of the diagonal elements 

- If square matrix `\(\mathbf{A}\)` is 
`$$\mathbf{A}= 
\begin{bmatrix}
a_{11}&amp; a_{12} &amp;\cdots &amp; a_{1n} \\
a_{21}&amp; a_{22} &amp;\cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
a_{n1}&amp; a_{n2} &amp;\cdots &amp; a_{nn} 
\end{bmatrix}$$`

- Then its trace is 

`$$tr(\mathbf{A})= \sum_{i=1}^{n} a_{ii}$$`

---
	
# Matrix Operations

- Important properties of the trace are 

`$$tr(\mathbf{I_{n}})= n$$`
`$$tr(\mathbf{A}')=tr(\mathbf{A})$$`
`$$tr(\mathbf{A +B})=tr(\mathbf{A}) + tr(\mathbf{B})$$`
`$$tr(\alpha \mathbf{A})=\alpha tr(\mathbf{A})$$`
`$$tr(\mathbf{AB})=tr(\mathbf{BA})$$`
---

# Matrix Operations
## Marix Determinant

- The .red[determinant] is a scalar value associated with a square matrix

  - Helpful concept for several things in matrix algebra

  - For econometrics, most useful for solving systems of equations and finding inverse of a matrix

- For `\(2 \times 2\)` matrix `\(\mathbf{A}\)` 
`$$\mathbf{A}= 
\begin{bmatrix}
a_{11}&amp; a_{12} \\
a_{21}&amp; a_{22}  \\
\end{bmatrix}$$`

- The determinant is 
	
`$$|\mathbf{A}|=a_{11}a_{22} - a_{12}a_{21}$$`

---

# Matrix Operations

- For `\(3 \times 3\)` matrix `\(\mathbf{A}\)` 

`$$\mathbf{A}= 
\begin{bmatrix}
a_{11}&amp; a_{12} &amp; a_{13} \\
a_{21}&amp; a_{22} &amp; a_{23} \\
a_{31}&amp; a_{32} &amp; a_{33} \\
\end{bmatrix}$$`

- The determinant is

`$$|\mathbf{A}|=a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} +a_{13}a_{21}a_{32}$$`
	`$$-(a_{12}a_{21}a_{33} + a_{11}a_{23}a_{32} +a_{13}a_{22}a_{31})$$`

`$$=a_{11}(a_{22}a_{33} - a_{23}a_{32}) + a_{12}(a_{23}a_{31} -a_{21}a_{33} )  +a_{13}(a_{21}a_{32} - a_{22}a_{31} )$$`

---

# Matrix Operations

- For `\(n \times n\)` matrix `\(\mathbf{A}\)` the determinant is

`$$|\mathbf{A}|=a_{i1}c_{i1} + a_{i2}c_{i2} + \cdots + a_{in}c_{in} \text{   for choice of any row i}$$`

- Where

  - `\(a_{ij}\)` is the `\(ij\)` element of matrix `\(\mathbf{A}\)` 
  
  - `\(c_{ij}\)` is the `\(ij\)` .red[cofactor] of matrix `\(\mathbf{A}\)` defined as
`$$c_{ij} = (-1)^{i+j}|\mathbf{A}_{ij}|$$`
  - `\(|\mathbf{A}_{ij}|\)` is the .red[minor] of matrix `\(\mathbf{A}\)`
  
     - Determinant of the sub-matrix formed by deleting the `\(i\)`th row and `\(j\)`th column of `\(\mathbf{A}\)`

- Process is long and tedious for large matrices

---

# Matrix Operations

- Example of `\(3 \times 3\)` matrix

`$$\mathbf{A}= 
\begin{bmatrix}
1&amp; 2 &amp; 3 \\
4&amp; 5&amp;6   \\
7&amp; 8 &amp;9  
\end{bmatrix}$$`

- Choose any row to find cofactors and compute determinant

  - Does not matter which
	
- Let us expand along row 1

`$$|\mathbf{A}|=1(-1)^{1+1} 
\begin{vmatrix}
  5&amp;6   \\
 8 &amp;9  
\end{vmatrix}
+2(-1)^{1+2} 
\begin{vmatrix}
  4&amp;6   \\
 7 &amp;9  
\end{vmatrix}
+3(-1)^{1+3} 
\begin{vmatrix}
  4&amp;5   \\
 7 &amp;8  
\end{vmatrix}$$`

`$$|\mathbf{A}|= -3 +12 -9 = 0$$`

---

# Matrix Operations
## Matrix Inverse

- The .red[inverse] of a square matrix `\(\mathbf{A}\)` is defined such that

`$$\mathbf{A}\mathbf{A}^{-1} = \mathbf{A}^{-1}\mathbf{A} = \mathbf{I}$$`

- It is roughly the equivalent of taking the reciprocal in scalar math

  - But it is **not** generally the reciprocal of the elements of a matrix

- The formula for the inverse is 	
	
`$$\mathbf{A}^{-1}= \frac{1}{|\mathbf{A}|}
\begin{bmatrix}
c_{11}&amp; c_{12} &amp;\cdots &amp; c_{1n} \\
c_{21}&amp; c_{22} &amp;\cdots &amp; c_{2n} \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
c_{n1}&amp; c_{n2} &amp;\cdots &amp; c_{nn} 
\end{bmatrix}$$`

- where `\(c_{ij}\)` are the cofactors defined above

---

# Matrix Operations

- The inverse exists only when `\(|\mathbf{A}| \neq 0\)`

  - This is why it is important to know the determinant
  
  - In example above, inverse does not exist

      - We will see later that it is because the columns are linearly dependent

- A matrix that cannot be inverted is .red[singular]

- A matrix that has an inverse is .red[nonsingular]

- Inverse matrices have the following properties

`$$\mathbf{(\alpha A)^{-1} = \frac{1}{\alpha} A^{-1} }$$`
`$$\mathbf{(A')^{-1}} = \mathbf{(A^{-1})' }$$`
`$$\mathbf{(A^{-1})^{-1}} = \mathbf{A}$$`
`$$\mathbf{(AB)^{-1}= B^{-1}A^{-1} }$$`
---

# Linear Independence and Rank of a Matrix

- Now that we can manipulate matrices, we can move to more advanced topics

- Matrix algebra is useful for expressing and solving systems of equations

  - This is how we will use it in econometrics
	
- We will learn you can solve for the OLS estimator when regressors are linearly independent 	

  - They are not linear functions of one another
	
- To check linear independence, we use the concept of rank
- The .red[rank] of a matrix is the maximum number of independent rows or columns 	
	
  - For non-square matrices, the maximum rank is the lesser of the number or rows or columns

---

# Linear Independence and Rank of a Matrix
## Linear Independence

- A set of vectors are .red[linearly independent] if you cannot express any of them as linear functions the others

- Mathematically, suppose that `\(\mathbf{A}=\begin{bmatrix} \mathbf{a}_{1}&amp; \mathbf{a}_{2} &amp;\cdots &amp; \mathbf{a}_{m} \end{bmatrix}\)`

  - where `\(\mathbf{a}_{1}, \mathbf{a}_{2}, \cdots,\mathbf{a}_{m}\)` are `\(n \times 1\)` vectors 


- The vectors are independent if the only solution to 

`$$\alpha_{1}\mathbf{a}_{1}+ \alpha_{2}\mathbf{a}_{2}+ \cdots+\alpha_{m}\mathbf{a}_{m}= 0$$` 
 
- is 

`$$\alpha_{1} = \alpha_{2}= \cdots=\alpha_{m}= 0$$`
- If at least one `\(\alpha_{i} \neq 0\)`, then the vectors are linearly dependent

---

# Linear Independence and Rank of a Matrix
## Rank of a Matrix


- The .red[rank] of a matrix is the maximum number of linearly independent rows or columns

  - The rank of the rows will always equal the rank of the columns
  
  - If the number of rows is less than columns, the highest rank is the number of rows
  
  - Vice versa if the number of columns is less than the number of rows


- A matrix has .rank[full rank] if rank equals the minimum of the number of rows/columns

- In econometrics, we mostly deal with matrices with more rows than columns

  - So the matrix will be full rank if the rank equals the number of columns

- We will see later we need our matrix of regressors to have full rank

  - None of the regressors can be linear functions of each other (no multicollinearity)

---

# Linear Independence and Rank of a Matrix

- Some useful properties of the rank of a matrix

  - The rank of a matrix and transpose are the same
`$$rank(\mathbf{A'}) = rank(\mathbf{A})$$`

  - If `\(\mathbf{A}\)` is `\(n \times m\)` then 
`$$rank(\mathbf{A}) \le min(n,m)$$`

  - If `\(\mathbf{A}\)` is `\(n \times n\)` and `\(rank(\mathbf{A}) =n\)` then `\(\mathbf{A}\)` is nonsingular (invertible)
  
---

# Quadratic Forms and Positive Definite Matrices


- If `\(\mathbf{A}\)` is `\(n \times n\)` and symmetric, and `\(\mathbf{x}\)` is `\(n \times 1\)`, the .red[quadratic form] for `\(\mathbf{A}\)` is

`$$\mathbf{x'Ax}=
\begin{bmatrix}
x_{1}&amp; x_{2} &amp;\cdots &amp; x_{n} 
\end{bmatrix}
\begin{bmatrix}
a_{11}&amp; a_{12} &amp;\cdots &amp; a_{1n} \\
a_{21}&amp; a_{22} &amp;\cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
a_{n1}&amp; a_{n2} &amp;\cdots &amp; a_{nn} 
\end{bmatrix}
\begin{bmatrix}
x_{1}\\
 x_{2} \\
 \vdots \\
  x_{n} 
\end{bmatrix}$$`

`$$=\sum_{i=1}^n a_{ii}x_{i}^2 + 2\sum_{i=1}^n \sum_{j&gt;i}a_{ij}x_{i}x_{j}$$`

- A matrix is .red[positive definite] if for all `\(\mathbf{x} \neq 0\)`

`$$\mathbf{x'Ax} &gt; 0$$`
---

# Quadratic Forms and Positive Definite Matrices

- A matrix is .red[positive semidefinite] if for all `\(\mathbf{x} \neq 0\)`

`$$\mathbf{x'Ax} \ge 0$$`
- Positive definite matrices have diagonal elements that are strictly positive

- Positive semidefinite matrices have diagonal elements that are nonnegative

- Some other useful properties of positive definite/semidefinite matrices

  - If `\(\mathbf{A}\)` is positive definite, then `\(\mathbf{A}^{-1}\)` exists and is also positive definite
  
  - If `\(\mathbf{A}\)` is `\(n \times m\)`, then `\(\mathbf{A'A}\)` and `\(\mathbf{AA'}\)` are positive definite
  
  - If `\(\mathbf{A}\)` is `\(n \times m\)` and `\(rank(\mathbf{A}) = m\)` then `\(\mathbf{A'A}\)` is positive definite

- These concepts are used mostly for variance-covariance matrices in econometrics

---

# Idempotent Matrices

- An .red[idempotent] matrix is one that does not change when multiplied by itself

- Mathematically, `\(\mathbf{A}\)` is idempotent when

`$$\mathbf{AA} = \mathbf{A}$$`


- When we discuss OLS, we will work with the following idempotent matrices

  - Suppose `\(\mathbf{X}\)` is `\(n \times k\)` with full rank.  Define

`$$\mathbf{P} = \mathbf{X(X'X)^{-1}X'}$$`
`$$\mathbf{M} =\mathbf{I_{n}} - \mathbf{X(X'X)^{-1}X'}$$`

  - You can verify they are idempotent my multiplying each by itself

- Some important properties of idempotent matrices are

  - `\(rank(\mathbf{A}) = tr(\mathbf{A})\)`
  
  - `\(\mathbf{A}\)` is positive semidefinite

---

# Moments of Random Vectors
## Expected Value

- The .red[expected value] of a random matrix is the matrix of expected values

- If `\(\mathbf{X}\)` is an `\(n \times m\)` matrix, then

`$$\mathbf{E}(\mathbf{X})=
\begin{bmatrix}
\mathbf{E}(x_{11}) &amp; \mathbf{E}(x_{12}) &amp; \cdots &amp; \mathbf{E}(x_{1m})\\
\mathbf{E}(x_{21}) &amp; \mathbf{E}(x_{22}) &amp; \cdots &amp;\mathbf{E}(x_{2m})\\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
\mathbf{E}(x_{n1}) &amp; \mathbf{E}(x_{n2}) &amp; \cdots &amp;\mathbf{E}(x_{nm})\\
\end{bmatrix}$$`

- Properties of expected values are similar to those in scalar math

  - If `\(\mathbf{x}\)` is a random vector, `\(\mathbf{b}\)` is a nonrandom vector, and `\(\mathbf{A}\)` is a nonrandom matrix, then `\(\mathbf{E}(\mathbf{Ax+b}) = \mathbf{A}\mathbf{E}(\mathbf{x})+\mathbf{b}\)` 
  
  - If `\(\mathbf{X}\)` is a random matrix, and `\(\mathbf{B}\)` and `\(\mathbf{A}\)` are nonrandom matrices, then `\(\mathbf{E}(\mathbf{AXB}) = \mathbf{A}\mathbf{E}(\mathbf{X})\mathbf{B}\)` 
  
---

# Moments of Random Vectors
## Variance-Covariance Matrix

- The .red[variance-covariance matrix] of random vector `\(\mathbf{y}\)` has variances on the diagonal, covariances in the off-diagonal

- If `\(\mathbf{y}\)` is an `\(n \times 1\)` random vector, then

`$$var(\mathbf{y})= \mathbf{\sigma_{y}} = \mathbf{E[(y-E[y])(y-E[y])']}$$`
`$$=
\begin{bmatrix}
\text{var}(y_{1}) &amp; \text{cov}(y_{1},y_{2}) &amp; \cdots &amp;\text{cov}(y_{1},y_{n}) \\
\text{cov}(y_{2},y_{1}) &amp; \text{var}(y_{2}) &amp; \cdots &amp;\text{cov}(y_{2},y_{n}) \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
\text{cov}(y_{n},y_{1})  &amp; \text{cov}(y_{n},y_{2}) &amp; \cdots &amp;\text{var}(y_{n})\\
\end{bmatrix}$$`
---

# Moments of Random Vectors

- Useful properties of variance-covariance matrices are

  - If `\(\mathbf{a}\)` is a nonrandom vector, then `\(\text{var}(\mathbf{a'y}) =\mathbf{a'}\text{var}\mathbf{(y)a}\)` 
  
  - If `\(\text{var}(\mathbf{a'y})&gt;0\)` for all `\(\mathbf{a&gt;0}\)`, `\(\text{var}(\mathbf{y})\)` is positive definite
  
  - If `\(\mathbf{A}\)` is a nonrandom matrix, `\(\mathbf{b}\)` is a nonrandom vector, then `\(\text{var}(\mathbf{Ay + b}) =\mathbf{A'}\text{var}\mathbf{(y)A}\)` 
  
  - If `\(\text{var}(y_{j})=\sigma^{2}\)` for all `\(j=1,2,...,n\)`, and the elements of `\(\textbf{y}\)` are uncorrelated, then `\(\text{var}(\mathbf{y})=\sigma^{2}\mathbf{I_{n}}\)`
  
---

# Matrix Differentiation

- A scalar function of a vector is a single function with respect to several variables

  - A vector function is a set of one or more scalar functions, each with respect to several variables
  
  - We will not cover these

- Consider the scalar function `\(y = f(\mathbf{x}) =f(x_{1}, x_{2},...,x_{n})\)` 

  - The function takes the vector `\(\mathbf{x}\)` and returns a scalar
  
  - This is just another way to write a multivariate function 

- The derivative of this function is

`$$\frac{\partial f(\mathbf{x})}{\mathbf{x}}=
\begin{bmatrix}
\frac{\partial f(\mathbf{x})}{x_{1}} &amp; \frac{\partial f(\mathbf{x})}{x_{2}} &amp; \cdots &amp; \frac{\partial f(\mathbf{x})}{x_{n}}  
\end{bmatrix}$$`

---

# Matrix Differentiation

- We simply collect the derivative with respect to each element of `\(\mathbf{x}\)` in a vector

- Ex:  linear function of `\(\mathbf{x}\)`

  - Suppose `\(\mathbf{a}\)` is an `\(n \times 1\)` vector and 
`$$y = f(\mathbf{x}) = \mathbf{a'x} = \sum_{i=1}^{n} a_{i}x_{i}$$`


  - The derivative is 
`$$\frac{\partial f(\mathbf{x})}{\partial \mathbf{x}}=\frac{\partial \mathbf{a'x} }{\partial \mathbf{x}}= \mathbf{a'} =
\begin{bmatrix}
a_{1}&amp; a_{2}&amp; \cdots &amp; a_{n}
\end{bmatrix}$$`

---

# Matrix Differentiation

- Ex:  Quadratic form of `\(\mathbf{x}\)`

  - Suppose `\(\mathbf{A}\)` is an `\(n \times n\)` symmetric matrix.  The quadratic form is
`$$y = f(\mathbf{x}) = \mathbf{x'Ax} =\sum_{i=1}^n a_{ii}x_{i}^2 + 2\sum_{i=1}^n \sum_{j&gt;i}a_{ij}x_{i}x_{j}$$`

  - The derivative is 
 
`$$\frac{\partial f(\mathbf{x})}{\partial \mathbf{x}}=\frac{\partial \mathbf{x'Ax} }{\partial \mathbf{x}}= \mathbf{2x'A}$$`
---

# Linear Regression Model in Matrix Notation

- In undergraduate textbooks, the .red[population linear regression model] is written as

`$$y= \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \cdots + \beta_{k}x_{k} + u$$`

  - `\(y\)` and `\(x_{1},...,x_{k}\)` are observable random variables
  
  - `\(u\)` is an unobservable random variable
	
- We can write more compactly in vector form as

`$$y=  \mathbf{x}\boldsymbol{\beta}  + u$$`

  - `\(\mathbf{x}\)` is a `\(1 \times (k+1)\)` vector of independent variables 

      - There are `\(k\)` independent variables, plus an intercept


  - `\(\boldsymbol{\beta}\)` is a `\((k+1) \times 1\)` vector of slope parameters 


---

# Linear Regression Model in Matrix Notation

- Now suppose we take a random sample of `\(n\)` people from the population

- The population model holds for each member of the sample	

`$$y_{i}=  \mathbf{x_{i}}\boldsymbol{\beta}  + u_{i}, \forall i=1,...,n$$` 

- We can express this more compactly with full matrix notation

`$$\mathbf{y}=  \mathbf{X}\boldsymbol{\beta}  + \mathbf{u}$$`

  - `\(\mathbf{X}\)` is an `\(n \times (k+1)\)` matrix of observations on each regressor
  
  - `\(\boldsymbol{\beta}\)` is still a `\((k+1) \times 1\)` vector of slope parameters 
  
  - `\(\mathbf{y}\)` is an `\(n \times 1\)` vector of observations on the dependent variable
  
  - `\(\mathbf{u}\)` is an `\(n \times 1\)` vector of error terms

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
